{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddef1f7c-7fc8-4613-a7c2-dcd1a160e4f8",
   "metadata": {},
   "source": [
    "# Import Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa6ad74-f654-481b-9889-09244f75b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df07ecf-b77f-4157-9d07-13e1d871d089",
   "metadata": {},
   "source": [
    "# Keypoints using mediapipe Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4d8a9-29cf-4907-8361-edcdbd57db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37192080-dfd4-4403-b43d-e457218b267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"Makes detections on the image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : frame\n",
    "        Frame from cv2.VideoCapture\n",
    "    model : \n",
    "        Model used for making detections\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    image : frame\n",
    "        Frame from cv2.VideoCapture\n",
    "    results : mediapipe.python.solution_base.SolutionOutputs\n",
    "        Results from model\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = model.process(image) # Make prediction.\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d9552c-ad84-4b33-8575-3761b3a9e512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    \"\"\"Visualizes detections(landamrks) on image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : \n",
    "        Image on which visualization is performed\n",
    "    results : mediapipe.python.solution_base.SolutionOutputs\n",
    "        Results from model\n",
    "    \"\"\"\n",
    "    #mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe05a6-bac0-4c2e-aed5-335cd864bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "\n",
    "        # Show\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be9bf8-0666-4750-b1c2-6de343c6ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a88cde-db6d-4d2b-abaf-c642fefdcaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54734818-fd8a-4f38-8bc3-64284218dc52",
   "metadata": {},
   "source": [
    "# Extract Keypoints Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff27a9-ff6f-473c-b70c-ffca6f436a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both hands have 21 landmarks - key points on themself.\n",
    "len(results.right_hand_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a4b250-95c6-48e3-b7f9-1ffddf528273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pose has 33 landmarks.\n",
    "len(results.pose_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d51d5d-7832-485d-b855-bb2378cc078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face has 468 landmarks but here I will not using them.\n",
    "len(results.face_landmarks.landmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce1b15-4f0b-41ce-9197-1c037adf7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of them have 3 values x,y and z but pose has visibility also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b01fd5-f20b-463b-963b-c0394ee2c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    \"\"\"Takes and returns all values from results concatenated.\n",
    "        If some of body parts are not detected by camera than results do not have values for it,\n",
    "        therefore error occured if we try to access it. For that reason we set all zeros for its value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : mediapipe.python.solution_base.SolutionOutputs\n",
    "        Results of mediapipe model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        a np.array of all values of left hand and right hand from results\n",
    "    \"\"\"\n",
    "\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    #return np.concatenate([pose, lh, rh])\n",
    "    return np.concatenate([lh, rh]) # Using only landmarks from hands for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866a301-e78a-4292-bb72-b078b2174962",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbf5a99-e727-4fe9-af70-d35c6969ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427222c9-d325-4bff-b7eb-06ebd5810ac0",
   "metadata": {},
   "source": [
    "# Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92598f-efc2-4cbc-a2b0-827b8b5b8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Setting folders for collection for every word, action.\n",
    "For each word, 30 videos will be collected.\n",
    "Each video will be 75 frames long.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8594ffdc-9483-4415-831d-b3e654d69684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('MP_Data')\n",
    "\n",
    "# Actions that we detect\n",
    "actions = np.array(['ja', 'ti', 'moj', 'tvoj', 'mi', 'vi', 'oni', 'svi', 'niko',\n",
    "                   'uzmi', 'daj', 'ostavi', 'nemoj', 'mogu', 'nemogu', 'imam', 'nemam',\n",
    "                   'nista', 'kako', 'zasto', 'treba', 'netreba', 'zdravo', 'dobardan',\n",
    "                   'staradite', 'kakoste', 'staimanovo', 'hvala', 'izvinite', 'dovidjenja',\n",
    "                   'zaomije', 'hocu', 'necu'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Each video is going to be 75 frames in length (2.15 sec, on 30fps)\n",
    "sequence_length = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f6be8-cfd8-4534-b19d-9bb3dc1665a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb9e16-6d9d-46f7-b8a9-9ffebb2b9e54",
   "metadata": {},
   "source": [
    "# Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a2eaf-006e-4c98-ace0-62d4c000be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collecting keypoint values for every action in every video.\n",
    "For every video, there will npy files containing values for keypoints.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa1e248-c115-433e-b589-b837bb1e8ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "\n",
    "    # Loop through actions\n",
    "    for action in actions:\n",
    "        # Loop through sequnces\n",
    "        for sequence in range(no_sequences):\n",
    "            # Loop through every frame, sequence length\n",
    "            for frame_num in range(sequence_length):\n",
    "\n",
    "                # Reed feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Makde detections\n",
    "                image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_landmarks(image, results)\n",
    "\n",
    "                # Wait logic for every video\n",
    "                if frame_num == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                               (0,255,0), 4, cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    # Show\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1, cv2.LINE_AA)\n",
    "                    # Show\n",
    "                    cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "                # Export keypoints\n",
    "\n",
    "                keypoints = extract_keypoints(results)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "                    break\n",
    "                    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4a58fb-80c2-44b4-839c-5e44ed11272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a96be-a7bd-4ae9-864e-de15d0a6a99d",
   "metadata": {},
   "source": [
    "# Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a651328e-751f-45de-b16e-18261ff63227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Preprocessing data from the previous step.\n",
    "Creating feature data and labels.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297e6c2e-16ab-42bc-9759-2b59b8f54006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718cea8-d5cf-4ed5-b2d0-b8ef0e1261a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad4667-8e9a-4140-a967-a249202600a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [] # Feature data, X-data\n",
    "labels = [] # Y-data, target\n",
    "\n",
    "# Loop through actions\n",
    "for action in actions:\n",
    "    # Loop through videos\n",
    "    for sequence in range(no_sequences):\n",
    "        window = []\n",
    "        # Loop through frames\n",
    "        for frame_num in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res) # Will have 75 arrays and each array has 258(126 if using just hands) values(keypoints)\n",
    "        sequences.append(window) # Will have 990 videos (33 actions * 30 videos)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6285faf6-2472-482f-91b2-b1330a0adb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0331e415-42ea-45b2-8065-5c52bed596e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5620cfe6-711a-4810-b92c-c4800cfa00b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d88e53-c344-4306-b431-04741f29608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = np.reshape(labels_array, (990,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685333bd-c067-478d-93dd-bcef25b608ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa53bc-c089-493a-8112-c45750bf0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int) # Converts a class vector (integers) to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddf8e9-488a-4021-9579-2d3d231e9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac2708-cc35-4a76-9ed9-9ea679959f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[56].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5594e-c000-43bf-9ff3-ee53abe7f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving X and y for following use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7548edb-2f3e-4c93-a271-a70030cdf3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"X\", X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefd611-8603-4cb2-b9c5-485b0915c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8719ce6-c930-4fd9-93dc-babd9999e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4681d303-e40a-40a8-b6fa-639adbbe5626",
   "metadata": {},
   "source": [
    "# Build and Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c62eae-babd-4e84-80e9-51160bf5fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is built and trained in Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e05badf-099e-44eb-a57a-9a707d29a02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ​RandomForestClassifier is used from sklearn.ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "479c6f78-78b1-44a2-bb17-9f731a7eabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is trained with data that was saved in previous step and saved with joblib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202a35fd-eb31-47d9-b81b-5f747d13662e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f252dfc-c85e-4ec1-8da1-e66b2e57b038",
   "metadata": {},
   "source": [
    "# Load Model and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f78b6-8342-421e-a4a1-7fbdc2d21ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa575c-991b-4525-b6f4-6e975d92ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5504cd5-1b3f-4680-a268-b415860fea8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5f1575-413e-46fb-92f2-f5ef0358267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfaf07d-cb98-4ed7-b6dd-22cb09dfea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"RFC_2.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e31ab1-edce-4c60-9164-f4e1c8b29054",
   "metadata": {},
   "source": [
    "# Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0acd1-d54a-4b6c-b058-eb8b9cf49ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(\"X1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29897b2c-a28b-4df7-8ecc-8edef2632772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c267cc-d019-44e4-a8e5-d1fbf991eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"y1.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a989d04-0415-4754-b007-341cd50f35c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(990, 9450) # 75 * 126 = 9450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb4142-d630-4d0b-91c5-06cec0ddc90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9aa51-f91b-4bc2-8fae-cbf5a69521e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd7f6a-a368-42af-82fb-62013a75df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3158b7-84ac-4c57-819a-77033aea67d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(val_predictions[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e2cf1-b722-4374-b240-39d1c4a9322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030225e-ed15-4800-a988-fc7988a4303f",
   "metadata": {},
   "source": [
    "# Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ab9ab-269b-45cb-b4f4-9e497d58bf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Compute a confusion matrix for each class or sample.\n",
    "Compute class-wise multilabel confusion matrix to evaluate\n",
    "the accuracy of a classification, and output confusion matrices \n",
    "for each class or sample.\n",
    "\n",
    "Accuracy classification score.\n",
    "In multilabel classification, this function computes subset accuracy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e2bd5-9862-43bf-bb87-0d4f6413b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cacefb-fbf5-418f-8040-162e66036203",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db27c3-cf18-4922-a15b-35966557013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b5771-6ec5-4ce2-b1a7-522e01eca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc994bc-8f1b-4694-af61-cac5b743faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574a019-37fc-4817-9792-81d4b76bf1f8",
   "metadata": {},
   "source": [
    "# Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88188fd2-7b1b-4dc0-9c2b-98b308575ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection variables\n",
    "sequence = []\n",
    "sentence = [] # Sentence that will be shown, that contains predicted words(last five words).\n",
    "threshold = 0.7\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe mode\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detection\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "        # Draw landmarks\n",
    "        draw_landmarks(image, results)\n",
    "\n",
    "        # Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-75:]   # Last 75 frames\n",
    "\n",
    "        \n",
    "        # Predict\n",
    "        if len(sequence) == 75:\n",
    "            arr = np.expand_dims(sequence, axis=0) # Shape (1, 75, 258)  (1, 75, 126)\n",
    "            arr = np.reshape(arr, (1, arr.size))   # Shape (1, 19350) (1, 9450), input shape for the model must be 2D\n",
    "            res = model.predict(arr)[0]\n",
    "            #print(actions[np.argmax(res)])\n",
    "            \n",
    "\n",
    "        # Vizualize\n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                if len(sentence) > 0:\n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "\n",
    "        cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "\n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d4ca5-2d5e-40cb-977d-a0ba7658277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
